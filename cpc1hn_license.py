# -*- coding: utf-8 -*-
"""CPC1HN_license.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6HDCpcrBt04DN03Yaew7DjTBAeaB4wm
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import albumentations as A
from tqdm import tqdm
import shutil

# ======== CẤU HÌNH ========
INPUT_DIR = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/train"
OUTPUT_DIR = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/train/Augmented"

INPUT_IMAGES = os.path.join(INPUT_DIR, "images")
INPUT_LABELS = os.path.join(INPUT_DIR, "labels")
OUTPUT_IMAGES = os.path.join(OUTPUT_DIR, "images")
OUTPUT_LABELS = os.path.join(OUTPUT_DIR, "labels")

MAX_CLASS_ID = 1

os.makedirs(OUTPUT_IMAGES, exist_ok=True)
os.makedirs(OUTPUT_LABELS, exist_ok=True)

def yolo_to_bbox(label_line, img_w, img_h):
    cls, x, y, w, h = map(float, label_line.strip().split())
    if int(cls) > MAX_CLASS_ID:
        raise ValueError(f"Label class {int(cls)} > MAX_CLASS_ID {MAX_CLASS_ID}")
    x_center, y_center = x * img_w, y * img_h
    w, h = w * img_w, h * img_h
    x_min = x_center - w / 2
    y_min = y_center - h / 2
    x_max = x_center + w / 2
    y_max = y_center + h / 2
    return [x_min, y_min, x_max, y_max, int(cls)]

def bbox_to_yolo(box, img_w, img_h):
    x_min, y_min, x_max, y_max, cls = box
    x_center = (x_min + x_max) / 2 / img_w
    y_center = (y_min + y_max) / 2 / img_h
    w = (x_max - x_min) / img_w
    h = (y_max - y_min) / img_h
    return f"{cls} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}"

# ======== CÁC PHÉP BIẾN ĐỔI HÌNH HỌC ========
GEOMETRIC_AUGS = ["horizontal_flip", "rotation", "shear"]

def get_augmentations(aug_name, is_geom):
    AUGMENT_MAP = {
        "horizontal_flip": A.HorizontalFlip(p=1.0),
        "rotation": A.Rotate(limit=15, border_mode=cv2.BORDER_REFLECT, p=1.0),
        "shear": A.Affine(shear={"x": (-10, 10), "y": (-10, 10)}, fit_output=False, p=1.0),
        "brightness": A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0, p=1.0),
        "blur": A.GaussianBlur(blur_limit=(3, 5), sigma_limit=(0.1, 1.0), p=1.0),
        "noise": A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.4), p=1.0)
    }

    if is_geom:
        return A.Compose(
            [AUGMENT_MAP[aug_name]],
            bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])
        )
    else:
        return A.Compose([AUGMENT_MAP[aug_name]])

# ======== VÒNG LẶP XỬ LÝ ẢNH ========
for file_name in tqdm(os.listdir(INPUT_IMAGES)):
    if not file_name.lower().endswith((".jpg", ".jpeg", ".png")):
        continue

    image_path = os.path.join(INPUT_IMAGES, file_name)
    label_path = os.path.join(INPUT_LABELS, file_name.rsplit('.', 1)[0] + ".txt")

    image = cv2.imread(image_path)
    if image is None:
        print(f" Không thể đọc ảnh: {file_name}")
        continue

    h, w = image.shape[:2]

    if not os.path.exists(label_path):
        print(f" Không tìm thấy file label cho ảnh: {file_name}")
        continue

    with open(label_path, "r") as f:
        lines = [l.strip() for l in f.readlines() if l.strip()]

    if not lines:
        print(f" File label rỗng cho ảnh: {file_name}")
        continue

    bboxes = []
    class_labels = []
    skip_image = False

    for line in lines:
        try:
            box = yolo_to_bbox(line, w, h)
            bboxes.append(box[:4])
            class_labels.append(box[4])
        except Exception as e:
            print(f" Lỗi đọc nhãn trong {label_path}: {e}")
            skip_image = True
            break

    if skip_image or not bboxes:
        print(f" Bỏ qua ảnh do nhãn lỗi hoặc không hợp lệ: {file_name}")
        continue

    # ===== SAO CHÉP ẢNH GỐC & LABEL =====
    try:
        shutil.copy(image_path, os.path.join(OUTPUT_IMAGES, file_name))
        shutil.copy(label_path, os.path.join(OUTPUT_LABELS, os.path.basename(label_path)))
    except Exception as e:
        print(f" Lỗi sao chép file gốc cho {file_name}: {e}")
        continue

    # ===== TĂNG CƯỜNG DỮ LIỆU =====
    for aug_name in ["horizontal_flip", "rotation", "shear", "brightness", "blur", "noise"]:
        is_geom = aug_name in GEOMETRIC_AUGS
        transform = get_augmentations(aug_name, is_geom)

        try:
            if is_geom:
                transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)
                aug_bboxes = transformed["bboxes"]
                aug_classes = transformed["class_labels"]
            else:
                transformed = transform(image=image)
                aug_bboxes = bboxes
                aug_classes = class_labels

            aug_img = transformed["image"]

            new_img_name = file_name.rsplit('.', 1)[0] + f"_{aug_name}.jpg"
            new_lbl_name = new_img_name.replace(".jpg", ".txt")

            cv2.imwrite(os.path.join(OUTPUT_IMAGES, new_img_name), aug_img)

            with open(os.path.join(OUTPUT_LABELS, new_lbl_name), "w") as f:
                for box, cls in zip(aug_bboxes, aug_classes):
                    x_min, y_min, x_max, y_max = box
                    yolo_line = bbox_to_yolo([x_min, y_min, x_max, y_max, cls], aug_img.shape[1], aug_img.shape[0])
                    f.write(yolo_line + "\n")

        except Exception as e:
            print(f" Augmentation '{aug_name}' thất bại cho ảnh {file_name}: {e}")

!pip install ultralytics

yaml_content = """
train: /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/train/images
val: /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/valid/images
test: /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/test/images

nc: 2
names: ['bienoto','biensoxe']
"""

with open('/content/drive/MyDrive/CPC1HN/data.yaml', 'w') as f:
    f.write(yaml_content)

model=YOLO('/content/drive/MyDrive/CPC1HN/yolov5s.pt')

from ultralytics import YOLO

model=YOLO('/content/drive/MyDrive/CPC1HN/best.pt')

model.train(
    data='/content/drive/MyDrive/CPC1HN/data.yaml',
    epochs=30,
    imgsz=640,
    batch=10,
    patience=10,
    project='/content/drive/MyDrive/CPC1HN/YOLOv5-Runs',
    name='finetune_biensoxe',
    exist_ok=True
)

model.val(data='/content/drive/MyDrive/CPC1HN/data.yaml', split='test')

test_dir = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/train/images"

results = model.predict(
    source=test_dir,
    save=True,
    save_txt=True,
    project="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/predict/train",
    name="images",
    exist_ok=True
)

test_dir = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/valid/images"

results = model.predict(
    source=test_dir,
    save=True,
    save_txt=True,
    project="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/predict/valid",
    name="images",
    exist_ok=True
)

import cv2
import matplotlib.pyplot as plt

def predict_one_image(model, image_path, conf=0.25):
    results = model.predict(source=image_path, conf=conf, save=False, verbose=False)

    result = results[0]
    img = result.orig_img.copy()

    # Vẽ bounding boxes
    for box in result.boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        cls_id = int(box.cls[0])
        conf_score = float(box.conf[0])
        label = f"{model.names[cls_id]} {conf_score:.2f}"

        # Vẽ hộp + nhãn
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img, label, (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    # Hiển thị ảnh
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(8, 6))
    plt.imshow(img_rgb)
    plt.axis('off')
    plt.show()

    return result

img_path = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/test/images/0b70fd7b-9907-456f-98eb-b144a4c65eb7_png.rf.d828a544b3a7838b89435f59c54951ea.jpg"
result = predict_one_image(model, img_path)

import os
import cv2

def yolo_to_box(x_center, y_center, width, height, img_w, img_h):
    x1 = int((x_center - width / 2) * img_w)
    y1 = int((y_center - height / 2) * img_h)
    x2 = int((x_center + width / 2) * img_w)
    y2 = int((y_center + height / 2) * img_h)
    return max(0, x1), max(0, y1), min(img_w, x2), min(img_h, y2)

def crop_from_yolo(image_dir, label_dir, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]

    for img_name in image_files:
        img_path = os.path.join(image_dir, img_name)
        base_name = os.path.splitext(img_name)[0]
        label_path = os.path.join(label_dir, base_name + ".txt")

        if not os.path.exists(label_path):
            print(f"Không tìm thấy label: {img_name}")
            continue

        img = cv2.imread(img_path)
        if img is None:
            print(f" Lỗi khi đọc ảnh: {img_path}")
            continue

        h, w = img.shape[:2]

        with open(label_path, 'r') as f:
            lines = f.readlines()

        for i, line in enumerate(lines):
            parts = line.strip().split()
            if len(parts) != 5:
                print(f" Dòng không hợp lệ trong {label_path}: {line}")
                continue
            _, x, y, bw, bh = map(float, parts)
            x1, y1, x2, y2 = yolo_to_box(x, y, bw, bh, w, h)
            crop = img[y1:y2, x1:x2]

            if len(lines) == 1:
                crop_name = f"{base_name}.png"
            else:
                crop_name = f"{base_name}_crop_{i}.png"

            cv2.imwrite(os.path.join(save_dir, crop_name), crop)
            print(f"Đã crop và lưu: {crop_name}")

crop_from_yolo(
    image_dir="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/predict/valid/images",
    label_dir="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/predict/valid/images/labels",
    save_dir="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/valid/images"
)

!pip install onnxruntime

pip install fast-plate-ocr[onnx-gpu]

!pip install tensorflow[and-cuda]

import onnxruntime as ort
print(ort.get_available_providers())

# Commented out IPython magic to ensure Python compatibility.
# Disable TF disable all debugging logs
# %env TF_CPP_MIN_LOG_LEVEL=3
# Use TensorFlow as Keras backned (JAX and PyTorch are also supported!)
# %env KERAS_BACKEND=tensorflow

m = LicensePlateRecognizer('cct-xs-v1-global-model')

!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/colombia_dataset_example.zip && unzip -q colombia_dataset_example.zip -d colombia_dataset

# Download the .keras model used for fine-tuning
!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/cct_xs_v1_global.keras
# Download the plate config. This defines how license plate images and text should be preprocessed for OCR
# Although you can modify this to suit your needs, since we will be fine-tuning, we will use the exact same
# config that was used originally for training cct_xs_v1_global model
!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/cct_xs_v1_global_plate_config.yaml
# Download also the model config, since we will fine-tuning above model, we will download the same config that
# was used to build and train originally the cct_xs_v1_global model
!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/cct_xs_v1_global_model_config.yaml

pip install fast-plate-ocr[train]

# Check the train annotations
!fast-plate-ocr validate-dataset \
  --annotations-file ./colombia_dataset/train_annotations.csv \
  --plate-config-file cct_xs_v1_global_plate_config.yaml

# Check the validation annotations
!fast-plate-ocr validate-dataset \
  --annotations-file ./colombia_dataset/valid_annotations.csv \
  --plate-config-file cct_xs_v1_global_plate_config.yaml

!fast-plate-ocr train-recognizer \
  --train ./train.csv \
  --val ./val.csv \
  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \
  --output-dir ./output_recognizer

import pandas as pd

# Đọc CSV với mã hóa UTF-8 có BOM (thường là nguyên nhân gây lỗi)
df = pd.read_csv('/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/train/annotation.csv', encoding='utf-8-sig')

# In ra tên cột để kiểm tra
print(df.columns.tolist())

!fast-plate-ocr validate-dataset \
  --annotations-file /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/train/annotations.csv \
  --plate-config-file cct_xs_v1_global_plate_config.yaml

!fast-plate-ocr validate-dataset \
  --annotations-file /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/valid/annotations.csv \
  --plate-config-file cct_xs_v1_global_plate_config.yaml

!fast-plate-ocr dataset-stats \
  --annotations /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/train/annotations.csv \
  --plate-config-file cct_xs_v1_global_plate_config.yaml

# Commented out IPython magic to ensure Python compatibility.
# We invoke it this way so we can visualize it properly in this notebook
# %matplotlib inline
# %run -m fast_plate_ocr.cli.visualize_augmentation -- \
      --img-dir /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/train/images \
      --columns 2 \
      --rows 4 \
      --show-original \
      --num-images 50 \
      --plate-config-file cct_xs_v1_global_plate_config.yaml

import albumentations as A
import cv2

A.save(
    A.Compose(
        [
            A.Affine(
                translate_percent=(-0.02, 0.02),
                scale=(0.75, 1.10),
                rotate=(-15, 15),
                border_mode=cv2.BORDER_CONSTANT,
                fill=(0, 0, 0),
                shear=(0.0, 0.0),
                p=0.75,
            ),
            A.RandomBrightnessContrast(brightness_limit=0.10, contrast_limit=0.10, p=0.5),
            A.OneOf(
                [
                    A.HueSaturationValue(
                        hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=10, p=0.7
                    ),
                    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.3),
                ],
                p=0.3,
            ),
            A.RandomGamma(gamma_limit=(95, 105), p=0.20),
            A.ToGray(p=0.05),
            A.OneOf(
                [
                    A.GaussianBlur(sigma_limit=(0.2, 0.5), p=0.5),
                    A.MotionBlur(blur_limit=(3, 3), p=0.5),
                ],
                p=0.2,
            ),
            A.OneOf(
                [
                    A.GaussNoise(std_range=(0.01, 0.03), p=0.2),
                    A.MultiplicativeNoise(multiplier=(0.98, 1.02), p=0.1),
                    A.ISONoise(intensity=(0.005, 0.02), p=0.1),
                    A.ImageCompression(quality_range=(55, 90), p=0.1),
                ],
                p=0.3,
            ),
            A.OneOf(
                [
                    A.CoarseDropout(
                        num_holes_range=(1, 14),
                        hole_height_range=(1, 5),
                        hole_width_range=(1, 5),
                        p=0.2,
                    ),
                    A.PixelDropout(dropout_prob=0.02, p=0.3),
                    A.GridDropout(ratio=0.3, fill="random", p=0.3),
                ],
                p=0.5,
            ),
        ]
    ),
    filepath_or_buffer="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/custom_augmentation.yaml",
    data_format="yaml",
)

!fast-plate-ocr valid \
  --model ./cct_xs_v1_global.keras \
  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \
  --annotations /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/valid/annotations.csv

!fast-plate-ocr train \
  --model-config-file ./cct_xs_v1_global_model_config.yaml \
  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \
  --annotations /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/train/annotations.csv \
  --val-annotations /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/valid/annotations.csv \
  --augmentation-path /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/custom_augmentation.yaml \
  --epochs 40 \
  --batch-size 10 \
  --output-dir /content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/trained_models/ \
  --weights-path cct_xs_v1_global.keras \
  --label-smoothing 0.0 \
  --weight-decay 0.0005 \
  --lr 0.0005

best_keras_model = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/trained_models/2025-07-31_03-03-05/ckpt-epoch_12-acc_0.950.keras"
exported_onnx = best_keras_model.replace(".keras", ".onnx")
!fast-plate-ocr export \
  --format onnx \
  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \
  --simplify \
  --model {best_keras_model}

from fast_plate_ocr import LicensePlateRecognizer

exported_onnx = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/trained_models/2025-07-31_03-03-05/ckpt-epoch_12-acc_0.950.onnx"

plate_recognizer = LicensePlateRecognizer(
    onnx_model_path=exported_onnx,
    plate_config_path="./cct_xs_v1_global_plate_config.yaml",
)

import cv2
image_path = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/cropped/test/images/0b70fd7b-9907-456f-98eb-b144a4c65eb7_png.rf.d828a544b3a7838b89435f59c54951ea.png"
image = cv2.imread(image_path)
result = plate_recognizer.run(image)
print("Kết quả nhận diện:")
print(result)

import cv2
import numpy as np

def crop_plate_from_result(img_path, result):
    img = cv2.imread(img_path)
    if img is None:
        print("Lỗi khi đọc ảnh.")
        return None

    h, w = img.shape[:2]

    if result.boxes is None or len(result.boxes) == 0:
        print("Không tìm thấy box nào.")
        return None

    xyxy = result.boxes.xyxy[0].cpu().numpy().astype(int)
    x1, y1, x2, y2 = xyxy

    cropped = img[y1:y2, x1:x2]
    return cropped

img_path = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/test/images/0b70fd7b-9907-456f-98eb-b144a4c65eb7_png.rf.d828a544b3a7838b89435f59c54951ea.jpg"
result = predict_one_image(model, img_path)

cropped_plate = crop_plate_from_result(img_path, result)

if cropped_plate is not None:
    ocr_result = plate_recognizer.run(cropped_plate)
    print("Kết quả OCR:", ocr_result)
else:
    print("Không crop được biển số.")

import cv2
import matplotlib.pyplot as plt
import numpy as np
from ultralytics import YOLO
from fast_plate_ocr import LicensePlateRecognizer

# --- Load models ---
yolo_model = YOLO('/content/drive/MyDrive/CPC1HN/best.pt')
plate_recognizer = LicensePlateRecognizer(
    onnx_model_path="/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/trained_models/2025-07-31_03-03-05/ckpt-epoch_12-acc_0.950.onnx",
    plate_config_path="cct_xs_v1_global_plate_config.yaml",
)

def predict_and_visualize(image_path, conf=0.25):
    results = yolo_model.predict(source=image_path, conf=conf, save=False, verbose=False)
    result = results[0]
    img = result.orig_img.copy()

    if result.boxes is None or len(result.boxes) == 0:
        print("Không tìm thấy đối tượng nào.")
        return

    for box in result.boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
        cls_id = int(box.cls[0])
        conf_score = float(box.conf[0])

        # Crop biển số
        cropped_plate = img[y1:y2, x1:x2]

        # OCR
        ocr_result = plate_recognizer.run(cropped_plate)
        vehicle_type = "motorbike" if cls_id == 1 else "car"

        print({
            "plate": ocr_result,
            "box": [x1, y1, x2, y2],
            "vehicle_type": vehicle_type,
        })

        # Vẽ lên ảnh
        label = f"{ocr_result} ({vehicle_type})"
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img, label, (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

    # Hiển thị ảnh kết quả
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(12, 8))
    plt.imshow(img_rgb)
    plt.axis("off")
    plt.title("Biển số đã nhận diện")
    plt.show()

# --- Run ---
if __name__ == "__main__":
    image_path = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/test/images/0b70fd7b-9907-456f-98eb-b144a4c65eb7_png.rf.d828a544b3a7838b89435f59c54951ea.jpg"
    predict_and_visualize(image_path)

import os
import cv2
import matplotlib.pyplot as plt
from ultralytics import YOLO
from fast_plate_ocr import LicensePlateRecognizer

# Cấu hình
image_folder = "/content/drive/MyDrive/CPC1HN/CPC1HN_biensoxe/test_yolov5"


def predict_and_crop_one(image_path):
    result = model.predict(image_path, conf=0.25)[0]
    img = cv2.imread(image_path)
    h, w = img.shape[:2]

    if result.boxes is None or len(result.boxes.data) == 0:
        return None, img

    x1, y1, x2, y2 = result.boxes.xyxy[0].cpu().numpy().astype(int)
    x1, y1, x2, y2 = max(0,x1), max(0,y1), min(w,x2), min(h,y2)
    cropped = img[y1:y2, x1:x2]
    return cropped, img

# Chạy qua tất cả ảnh
for file_name in os.listdir(image_folder):
    if not file_name.lower().endswith(('.jpg', '.png')):
        continue

    img_path = os.path.join(image_folder, file_name)
    cropped_plate, full_img = predict_and_crop_one(img_path)

    if cropped_plate is not None:
        ocr_result = plate_recognizer.run(cropped_plate)
        print(f"[{file_name}] → OCR: {ocr_result}")

        # Hiển thị ảnh và text
        plt.figure(figsize=(8, 5))
        plt.imshow(cv2.cvtColor(full_img, cv2.COLOR_BGR2RGB))
        plt.title(f"{file_name} | Biển số: {ocr_result}")
        plt.axis('off')
        plt.show()

    else:
        print(f"[{file_name}] → Không crop được biển số.")